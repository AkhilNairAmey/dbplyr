<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Databases}
-->

```{r, echo = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
```

# Databases

As well as working with local in-memory data like data frames and data tables, dplyr can also work with remote on-disk data stored in databases.  Currently dplyr supports the three main open source databases (sqlite, mysql and postgresql), and google's bigquery.

Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle. The reason you'd want to use dplyr with a database is because either your data is already in a database (and you don't want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database.

Since R almost exclusively works with in-memory data, if you do have a lot of data in a database, you're not going to be able to pull it all down in R. Instead, you'll have to work with subsets or aggregates - dplyr aims to make that as easy to do as possible.

## Getting started

To experiement with databases, it's easiest to get started with SQLite because everything you need is included in the R package. You don't need to install anything else, and you don't need to deal with the hassle of setting up a database server. Using a SQLite database in dplyr is really easy. Just give it a path and give it the ok to create some data.

```{r}
my_db <- src_sqlite("my_db.sqlite3", create = T)
```

We'll load it up with the `hflights` data using the convenient `copy_to()` function. This is a quick and dirty way of getting data into a database, but it's not suitable for very large datasets (because all the data has to flow through R).

```{r}
hflights_sqlite <- copy_to(my_db, hflights, indexes = list(
  c("Year", "Month", "DayofMonth"), "UniqueCarrier"))
```

When used with the data manipulation functions described above, it works almost exactly the same as a database:

```{r}
hflights_sqlite
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
filter(hflights_sqlite, depDelay > 240)
arrange(hflights_sqlite, Year, Month, DayofMonth)
mutate(hflights_sqlite, speed = AirTime / Distance)
summarise(hflights_sqlite, delay = mean(DepTime))
```

The most important difference is all the expression in `select()`, `filter()`, `arrange()`, `mutate()`, and `summarise()` are translated into SQL so they can be run on the database. This translation is almost perfect for the most common operations but there are some limitations, which we'll discuss later.

## Lazyness

When working with databases, dplyr tries to be as lazy as possible. It's lazy in two ways:

* it never pulls data back to R unless you explicitly ask for it.

* it collects together all the operations that you've done to the dataset and sends them all at once to the database so that it has as much information as possible.

For example, if you do:

```{r}
c1 <- filter(hflights_sqlite, DepDelay > 0)
c2 <- select(c1, Year, Month, DayofMonth, UniqueCarrier, DepDelay, AirTime, Distance)
c3 <- mutate(c2, Speed = Distance / AirTime * 60)
c4 <- arrange(c3, Year, Month, DayofMonth, UniqueCarrier)
```

The database doesn't actually do anything. It's not until you ask for the data (e.g. by printing `c4`) that dplyr generates the SQL and requests the results from the database, and even then it only pulls down 10 rows. 

To pull down all the results use `collect()`, which returns a `tbl_df()`:

```{r}
collect(c4)
```

You can see the query dplyr has generated by looking at the `query` component of the object:

```{r}
c4$query
```

## SQL translation

When doing simple mathematical operations of the type that you usually do for filterng, creating new colums and summaries it's relatively straightforward to translate R code to SQL (or indeed to any other programming language). You can experiment with this translation using `translate_sql()`.  The following examples work through some basic differences between R and SQL.

```{r}
# In SQLite variable names are escaped by double quotes:
translate_sql(x)
# And strings are escaped by single quotes
translate_sql("x")

# Many functions have slightly different names
translate_sql(x == 1 && (y < 2 || z > 3))
translate_sql(x ^ 2 < 10)
translate_sql(x %% 2 == 10)

# R and SQL have different defaults for integers vs reals.
# In R, 1 is an real, and 1L is an integer
# In SQL, 1 is an integer, and 1 is a real
translate_sql(1)
translate_sql(1L)
```

For SQLite, dplyr knows how to convert the following function:

* basic math operators: `+`, `-`, `*`, `/`, `%%`, `^`
* math functions: `abs`, `acos`, `acosh`, `asin`, `asinh`, `atan`, `atan2`, `atanh`, `atn2`, `ceil`, `cos`, `cosh`, `cot`, `coth`, `degrees`, `difference`, `exp`, `floor`, `log`, `log10`, `sign`, `sin`, `sinh`, `sqrt`, `tan`, `tanh`
* logical comparisons: `<`, `<=`, `!=`, `>=`, `>`, `==`
* boolean operations: `&`, `|`, `!`, `xor`
* basic aggregations: `mean`, `sum`, `min`, `max`, `sd`, `var`,

Any function that dplyr does't know how to convert it leaves as is - that means if you want to use any other function that [SQLite provides](http://www.sqlite.org/lang_corefunc.html) you can use it as is.  Here a few examples:

```{r}
translate_sql(glob(x, y)) 
translate_sql(x %in% c(1, 2, 3))
```

The basic techniques underying the implementation of `translate_sql` are described in a http://adv-r.had.co.nz/dsl.html.

## Grouping

Unfortunately SQLite does not support all the same group by operations that R does. SQLite lacks what's known as window function, which are needed to implement grouped mutate and filter. This means that only really useful operation for grouped sqlite tables in `summarise()`:

```{r}
delays <- chain(hflights_sqlite,
  group_by(Dest, UniqueCarrier),
  summarise(n = n(), avg_delay = mean(ArrDelay)),
  ungroup(), # have to manually ungroup
  filter(n > 20),
  collect()
)
qplot(UniqueCarrier, avg_delay, data = delays)
```

## Do

The nuclear option. Slow, but allows you to do anything you can do in R.

Assumes that you have streaming algorithms available, e.g. as in biglm.

## Other databases

The overall workflow is similar regardless of what database you're connecting to.  The following sections go in to more details on the pecularities of each database engine.

## Postgresql

## MySQL

## Bigquery

## Database advice

If you don't already have a database, here's some advice from my experiences setting up and running all of them. SQLite is by far and the easiest to get setup up, but the downsides is that it it has relatively limited functionality for data anlaysis. PostgreSQL is not too much hard to use and has ever function you might concievably need. Don't bother with MySQL/MariaDB: it's a pain to set up and the documentation is subpar. Google bigquery might be a good fit if you have very large data, or you're willing to pay (a small amount of) money for someone else to look after your database.

