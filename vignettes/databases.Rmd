<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Databases}
-->

```{r, echo = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
```

# Databases

As well as working with local in-memory data like data frames and data tables, dplyr can also work with remote on-disk data stored in databases. Currently dplyr supports the three main open source databases (sqlite, mysql and postgresql), and google's bigquery.

Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle. The reason you'd want to use dplyr with a database is because either your data is already in a database (and you don't want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database.

Since R almost exclusively works with in-memory data, if you do have a lot of data in a database, you're not going to be able to pull it all down in R. Instead, you'll have to work with subsets or aggregates - dplyr aims to make that as easy to do as possible. If you're working with very large data, it's also likely that you'll need support to get the data into the database and to ensure it is set up correctly to achieve good performance. dplyr provides some simple tools to help with these tasks but they are no substiture for expert help.

The goal of dplyr is not to replace every SQL function with an R function. Instead it aims to make a simple interface to the most common types of SQL you write as an analyst. It removes the cognitive cost of switching between two languages and insulates you from the many minor differences between R and SQL that are most likely to trip you up. 

## Getting started

To experiement with databases, it's easiest to get started with SQLite because everything you need is included in the R package. You don't need to install anything else, and you don't need to deal with the hassle of setting up a database server. Using a SQLite database in dplyr is really easy. Just give it a path and give it the ok to create some data.

```{r}
my_db <- src_sqlite("my_db.sqlite3", create = T)
```

We'll load it up with the `hflights` data using the convenient `copy_to()` function. This is a quick and dirty way of getting data into a database, but it's not suitable for very large datasets (because all the data has to flow through R).

```{r}
hflights_sqlite <- copy_to(my_db, hflights, indexes = list(
  c("Year", "Month", "DayofMonth"), "UniqueCarrier"))
```

As you can see the `copy_to()` operation has additional arguments that allow you to supply indexes for the table. By default, `copy_to()` also executes the SQL ANALYZE operation on the table: this updates statistics about the table so that the database has accurate information when selecting optimisations.

When used with the data manipulation functions described above, it works almost exactly the same as a data frame:

```{r}
hflights_sqlite
select(hflights_sqlite, Year:DayofMonth, DepDelay, ArrDelay)
filter(hflights_sqlite, depDelay > 240)
arrange(hflights_sqlite, Year, Month, DayofMonth)
mutate(hflights_sqlite, speed = AirTime / Distance)
summarise(hflights_sqlite, delay = mean(DepTime))
```

The most important difference is all the expression in `select()`, `filter()`, `arrange()`, `mutate()`, and `summarise()` are translated into SQL so they can be run on the database. This translation is almost perfect for the most common operations but there are some limitations, which we'll discuss later.

## Lazyness

When working with databases, dplyr tries to be as lazy as possible. It's lazy in two ways:

* it never pulls data back to R unless you explicitly ask for it.

* it collects together all the operations that you've done to the dataset and sends them all at once to the database so that it has as much information as possible.

For example, if you do:

```{r}
c1 <- filter(hflights_sqlite, DepDelay > 0)
c2 <- select(c1, Year, Month, DayofMonth, UniqueCarrier, DepDelay, AirTime, Distance)
c3 <- mutate(c2, Speed = Distance / AirTime * 60)
c4 <- arrange(c3, Year, Month, DayofMonth, UniqueCarrier)
```

The database doesn't actually do anything. It's not until you ask for the data (e.g. by printing `c4`) that dplyr generates the SQL and requests the results from the database, and even then it only pulls down 10 rows. 

To pull down all the results use `collect()`, which returns a `tbl_df()`:

```{r}
collect(c4)
```

You can see the query dplyr has generated by looking at the `query` component of the object:

```{r}
c4$query
```

You can also ask SQLite to explain how it plans to execute the query with `explain_tbl()`. The output is explained in more detail on the [SQLite website](http://www.sqlite.org/eqp.html) and is helpful if you're trying to figure out what indexes are being used.

```{r}
explain_tbl(c4)
```

### Forcing computation

There are three ways to force the computation of a query:

* `collect()` executes the query and returns the results to R
* `compute()` executes the query and stores the results in a temporary table on the database
* `collapse()` turns the complex SQL expression generated by multiple data manipulation operations and turns it into a table expression.

You are most likely to use `collect()`. `compute()` and `collapse()` may be useful as performance optimisations if you have some SQL knowledge.

## SQL translation

When doing simple mathematical operations of the type that you usually do for filterng, creating new colums and summaries it's relatively straightforward to translate R code to SQL (or indeed to any other programming language). You can experiment with this translation using `translate_sql()`.  The following examples work through some basic differences between R and SQL.

```{r}
# In SQLite variable names are escaped by double quotes:
translate_sql(x)
# And strings are escaped by single quotes
translate_sql("x")

# Many functions have slightly different names
translate_sql(x == 1 && (y < 2 || z > 3))
translate_sql(x ^ 2 < 10)
translate_sql(x %% 2 == 10)

# R and SQL have different defaults for integers vs reals.
# In R, 1 is an real, and 1L is an integer
# In SQL, 1 is an integer, and 1 is a real
translate_sql(1)
translate_sql(1L)
```

For SQLite, dplyr knows how to convert the following function:

* basic math operators: `+`, `-`, `*`, `/`, `%%`, `^`
* math functions: `abs`, `acos`, `acosh`, `asin`, `asinh`, `atan`, `atan2`, `atanh`, `atn2`, `ceil`, `cos`, `cosh`, `cot`, `coth`, `degrees`, `difference`, `exp`, `floor`, `log`, `log10`, `sign`, `sin`, `sinh`, `sqrt`, `tan`, `tanh`
* logical comparisons: `<`, `<=`, `!=`, `>=`, `>`, `==`
* boolean operations: `&`, `|`, `!`, `xor`
* basic aggregations: `mean`, `sum`, `min`, `max`, `sd`, `var`,

Any function that dplyr does't know how to convert it leaves as is - that means if you want to use any other function that [SQLite provides](http://www.sqlite.org/lang_corefunc.html) you can use it as is.  Here a few examples:

```{r}
translate_sql(glob(x, y)) 
translate_sql(x %in% c(1, 2, 3))
```

The basic techniques underying the implementation of `translate_sql` are described in a http://adv-r.had.co.nz/dsl.html. It is built on top of R's parsing engine and has been carefully design to always generate correct sql. It also protects you against SQL injection attacks by correctly escaping strings and variable names as needed by the database that you're connecting to.

### Semantic translation

The goal of dplyr is to provide a semantic translation: to translate what you mean, not the precise details. The results will not be exactly the same: it's the database executing the code not R, and database programmers have different priorities to R core.  For example, in the R `mean()` actually loops through the data twice in order to get a higher level of numerical accuracy at the cost of being twice as slow. R's `mean()` also provides a `trim` option for computing trimmed means, and databases do not. Databases automatically drop NULLs (their equivalent of missing values) whereas in R you have to ask nicely. All this means the essense of simple calls like `mean(x)` will be translated accurately, but more complicated calls like `mean(x, trim = 0.5, na.rm = TRUE)` will not be.

### Raw SQL

If dplyr doesn't support the SQL operation that you need, you can also create a `tbl` object from arbitrary SQL:

```{r}
tbl(my_db, sql("SELECT * FROM hflights"))
```

This will behave exactly like a `tbl` created directly from a database table - you can still `filter()`, `arrange()`, `select()`, `mutate()` and `summarise()` it.

### Performance considerations

By and large dplyr tries to prevent you from accidentally performing expensive query operations. This is why `ncol()` is always `NA`: in general, there's no way to determine how many rows a query will return unless you actually run the whole thing. Similarly, when you print a database tbl, it will only run the query enough to get the first 10 rows. A related limitation is that most remote tbls don't support `tail()`: you can't find the last rows without executing the whole query. 

## Grouping

Unfortunately SQLite does not support all the same group by operations that R does. SQLite lacks what's known as window function, which are needed to implement grouped mutate and filter. This means that only really useful operation for grouped sqlite tables in `summarise()`:

```{r}
delays <- chain(hflights_sqlite,
  group_by(Dest, UniqueCarrier),
  summarise(n = n(), avg_delay = mean(ArrDelay)),
  ungroup(), # have to manually ungroup
  filter(n > 20),
  collect()
)
qplot(UniqueCarrier, avg_delay, data = delays)
```

However, it's often possible to simulate grouped filters and mutates using self joins, where you join the original table with a summarised version. For example, imagine for each day, we want to find the most delayed flight. If SQLite supported window functions we could write:

```{r, eval = FALSE}
daily <- group_by(hflights_sqlite, Year, Month, DayofMonth)
subset(daily, AvgDelay == max(AvgDelay))
```

Instead we can solve the problem in two steps. We first find out the maximum delay for each day, and then we construct a semi-join to find only the records that match the date and the maximum value. We can do similar tricks for grouped mutates using 

```{r}
max_delay <- summarise(daily, max_delay = max(ArrDelay))
semi_join(mutate(daily, max_delay = ArrDelay), max_delay)
```

## Do

The nuclear option. Slow, but allows you to do anything you can do in R.

Assumes that you have streaming algorithms available, e.g. as in biglm.

## Other databases

The overall workflow is similar regardless of what database you're connecting to.  The following sections go in to more details on the pecularities of each database engine. All of these databases follow a client-server model - as well as your computer which is connecting to the databse, there is another computer actually running it (that might be your computer but usually isn't). Getting one of these databases setup up is beyond the scope of this article, but there are plenty of tutorials available on the web.

### Postgresql

`src_postgres()` has five arguments: `dbname`, `host`, `port`, `user` and `password`. If you are running a local postgresql database with the default settings you'll only need `dbname`, but in most cases you'll need all five. dplyr uses the RPostgreSQL package to connect to postgres databases, and unfortunately it does not currently support SSL connections.

For example, the following code allows me to connect to a local postgresql database that contains a copy of the `hflights` data:

```{r}
hflights <- tbl(src_postgres("hflights"), "hflights")
```

Postgres is a considerably more powerful database than SQLite.  It has:

* a much wider [range of functions](http://www.postgresql.org/docs/9.3/static/functions.html) built in to the database

* support for [window functions](http://www.postgresql.org/docs/9.3/static/tutorial-window.html), which allow grouped subset and mutates to work.

The following examples shows the grouped filter and mutate possible with PostgreSQL. The SQL generated from the grouped filter is quite complex because you can't filter on window functions directly; instead they have to go in a subquery.

```{r}
daily <- group_by(hflights, Year, Month, DayofMonth)

# Find the most and least delayed flight each day
bestworst <- filter(daily, ArrDelay == min(ArrDelay) || 
  ArrDelay == max(ArrDelay))
bestworst$query

# Rank each flight within a daily
ranked <- mutate(daily, rank = rank(desc(ArrDelay)))
bestworst$query
```

### MySQL and MariaDB

You can connect to MySQL and MariaDB (a recent fork of MySQL) through `src_mysql()`, mediated by the [RMySQL](https://github.com/jeffreyhorner/RMySQL) db package. Like PostgreSQL, you'll need to provide a database name, username, password, host, and port.

In terms of functionality, MySQL lies somewhere between SQLite and PostgreSQL. It provides a wide range of [built-in functions](http://dev.mysql.com/doc/refman/5.0/en/functions.html), but it does not support windowed functions (so you can't do grouped mutates and filters).

### Bigquery

Bigquery is a hosted database server provided by 

To create a bigquery project:

1. Create account with google's [cloud platform](https://cloud.google.com)
2. Once you've logged in, create a new project
3. Give your project a name (it doesn't matter what). 
4. Click on APIs & Auth, then APIs and bigquery API access on 
5. Go to the [bigquery console](https://bigquery.cloud.google.com/)
6. Create a new dataset: this is equivalent to a database and will store the tables you create for a given project.

After you create a bigquery src with your project and dataset, your web browser will open and ask you to authenticate the bigrquery package to access your data. Currently the authentication data is not cached anywhere so you'll need to repeat this process every time you connect, but a future version of bigrquery will provide a secure way of locally storing your credentials so you only need to do that once.

Bigquery supports only a single SQL statement: [SELECT](https://developers.google.com/bigquery/query-reference). But luckily this is all you need for data analysis, and within SELECT bigquery provides comprehensive coverage at a similar level to postgresql.

## Picking a database

If you don't already have a database, here's some advice from my experiences setting up and running all of them. SQLite is by far and the easiest to get setup up, but the downsides is that it it has relatively limited functionality for data anlaysis. PostgreSQL is not too much hard to use and has ever function you might concievably need. Don't bother with MySQL/MariaDB: it's a pain to set up and the documentation is subpar. Google bigquery might be a good fit if you have very large data, or you're willing to pay (a small amount of) money for someone else to look after your database.

